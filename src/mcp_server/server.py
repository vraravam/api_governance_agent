#!/usr/bin/env python3
"""
MCP Server for API Governance Tool

Exposes API governance capabilities as MCP tools for IDE-based agents.
Uses stdio transport for local development and IDE integration.
"""

import io
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List

# ============================================
# FIX WINDOWS UNICODE ENCODING ISSUE
# ============================================
# On Windows, Python defaults to CP-1252 encoding which can't handle emojis
# This causes UnicodeEncodeError when logging messages with emojis like üîç
# Force UTF-8 mode for all I/O operations
# Note: sys.platform == 'win32' is True for ALL Windows versions (32-bit, 64-bit, Win10, Win11)
if sys.platform == "win32" or sys.platform.startswith("win"):
    # Set environment variable for Python UTF-8 mode
    os.environ["PYTHONIOENCODING"] = "utf-8"

    # Reconfigure stdout and stderr to use UTF-8 encoding
    if hasattr(sys.stdout, "buffer"):
        sys.stdout = io.TextIOWrapper(
            sys.stdout.buffer, encoding="utf-8", errors="replace", line_buffering=True
        )
    if hasattr(sys.stderr, "buffer"):
        sys.stderr = io.TextIOWrapper(
            sys.stderr.buffer, encoding="utf-8", errors="replace", line_buffering=True
        )
# ============================================

# Add project root to path for imports
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from mcp.server.fastmcp import FastMCP
except ImportError as e:
    print(
        f"Error: fastmcp import failed: {e}. Run: pip install fastmcp", file=sys.stderr
    )
    sys.exit(1)

from autofix.category_manager import CategoryManager
from autofix.engine import AutoFixEngine
from autofix.proposer import ProposedFix
from autofix.review_gate import ReviewState
from autofix.subcategory_manager import SubcategoryManager
from engines.arch_unit_engine import ArchUnitEngine
from mcp_server.output_normalizer import OutputNormalizer
from mcp_server.tool_schemas import (
    CreateGovernancePROutput,
    GovernanceSummaryInput,
    GovernanceSummaryOutput,
    ProposeFixesOutput,
    ProposedFixInfo,
    ReviewFixesOutput,
    ValidateArchitectureInput,
    ValidateArchitectureOutput,
    ValidateOpenAPIInput,
    ValidateOpenAPIOutput,
)
from scanner import GovernanceScanner
from scanner.project_detector import ProjectDetector
from utils.logger import setup_logger

# Setup logger with UTF-8 support
logger = setup_logger("mcp_server")

# Auto-fix imports


# ========================================
# SPECTRAL CLI CHECK AND AUTO-INSTALL
# ========================================
def check_and_install_spectral() -> bool:
    """
    Check if Spectral CLI is installed, and offer to install if missing.

    Returns:
        True if Spectral is available, False otherwise
    """
    import shutil

    # Check if spectral is in PATH
    spectral_path = shutil.which("spectral")
    if spectral_path:
        try:
            result = subprocess.run(
                ["spectral", "--version"], capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                version = result.stdout.strip()
                logger.info(f"‚úì Spectral CLI found: {version}")
                return True
        except Exception as e:
            logger.warning(f"Spectral found but version check failed: {e}")

    # Spectral not found - attempt auto-install
    logger.warning("‚ö†Ô∏è  Spectral CLI not found in PATH")
    logger.info("üîß Attempting to install Spectral CLI automatically...")

    # Check if npm is available
    npm_path = shutil.which("npm")
    if not npm_path:
        logger.error("‚ùå npm not found - cannot install Spectral automatically")
        logger.error("üìã Manual installation required:")
        logger.error("   1. Install Node.js and npm from: https://nodejs.org/")
        logger.error("   2. Run: npm install -g @stoplight/spectral-cli")
        return False

    # Attempt npm install
    try:
        logger.info("Installing: npm install -g @stoplight/spectral-cli")
        logger.info("(This may take a minute...)")

        # Try standard installation first
        result = subprocess.run(
            ["npm", "install", "-g", "@stoplight/spectral-cli"],
            capture_output=True,
            text=True,
            timeout=120,  # 2 minutes timeout
        )

        # Check if installation failed due to registry issues
        if result.returncode != 0:
            stderr_lower = result.stderr.lower()

            # Detect npm registry issues (ECONNREFUSED, localhost registry, etc.)
            if any(
                error in stderr_lower
                for error in [
                    "econnrefused",
                    "localhost",
                    "connection refused",
                    "fetch error",
                ]
            ):
                logger.warning(
                    "‚ö†Ô∏è  npm registry issue detected (local registry not available)"
                )
                logger.info("üîß Retrying with public npm registry...")

                # Retry with explicit registry flag to use public npm registry
                result = subprocess.run(
                    [
                        "npm",
                        "install",
                        "-g",
                        "@stoplight/spectral-cli",
                        "--registry=https://registry.npmjs.org/",
                    ],
                    capture_output=True,
                    text=True,
                    timeout=120,
                )

                if result.returncode != 0:
                    logger.error(
                        f"‚ùå Spectral installation failed even with public registry: {result.stderr}"
                    )
                    logger.error("üìã Try manual installation with public registry:")
                    logger.error(
                        "   npm install -g @stoplight/spectral-cli --registry=https://registry.npmjs.org/"
                    )
                    logger.error("üìã Or fix your npm registry configuration:")
                    logger.error(
                        "   npm config set registry https://registry.npmjs.org/"
                    )
                    return False

        if result.returncode == 0:
            logger.info("‚úÖ Spectral CLI installed successfully!")

            # Verify installation
            spectral_path = shutil.which("spectral")
            if spectral_path:
                logger.info(f"‚úì Spectral is now available at: {spectral_path}")
                return True
            else:
                logger.warning("‚ö†Ô∏è  Spectral installed but not found in PATH")
                logger.warning("You may need to restart your terminal or IDE")
                return False
        else:
            logger.error(f"‚ùå Spectral installation failed: {result.stderr}")
            logger.error(
                "üìã Try manual installation: npm install -g @stoplight/spectral-cli"
            )
            return False

    except subprocess.TimeoutExpired:
        logger.error("‚ùå Spectral installation timed out")
        logger.error(
            "üìã Try manual installation: npm install -g @stoplight/spectral-cli"
        )
        return False
    except Exception as e:
        logger.error(f"‚ùå Spectral installation error: {e}")
        logger.error(
            "üìã Try manual installation: npm install -g @stoplight/spectral-cli"
        )
        return False


# ========================================
# GOVERNANCE REPORT PATHS
# ========================================
def get_governance_paths(project_path: Path) -> Dict[str, Path]:
    """
    Get standardized paths for governance reports.
    All reports go into build/governance/ directory which can be cleaned automatically.
    Only one copy of each report is maintained (overwrites on each scan).

    Args:
        project_path: Root path of the project being scanned

    Returns:
        Dict with all governance-related paths
    """
    # Use build/governance for all reports (ignored by most .gitignore files)
    governance_dir = project_path / "build" / "governance"
    governance_dir.mkdir(parents=True, exist_ok=True)

    return {
        "governance_dir": governance_dir,
        "governance_report": governance_dir / "governance-report.json",
        "api_report": governance_dir / "api-violations.json",
        "arch_report": governance_dir / "arch-violations.json",
        "arch_temp": governance_dir / "arch-violations-temp.json",
        "combined_report": governance_dir / "combined-report.md",
        "api_report_md": governance_dir / "api-governance-report.md",
        "fix_instructions_dir": governance_dir / "copilot-fixes",
        "fix_instructions_md": governance_dir
        / "copilot-fixes"
        / "copilot-fix-instructions.md",
        "fix_instructions_json": governance_dir
        / "copilot-fixes"
        / "copilot-fix-instructions.json",
    }


# ========================================

# Initialize MCP server
mcp = FastMCP("API Governance Server")

# ========================================
# SERVER STARTUP: CHECK DEPENDENCIES
# ========================================
logger.info("=" * 80)
logger.info("üöÄ API Governance Server Starting...")
logger.info("=" * 80)

# Check and install Spectral CLI if needed
spectral_available = check_and_install_spectral()
if not spectral_available:
    logger.warning("‚ö†Ô∏è  Server starting WITHOUT Spectral CLI")
    logger.warning("‚ö†Ô∏è  OpenAPI spec violations will NOT be detected")
    logger.warning("‚ö†Ô∏è  Install Spectral: npm install -g @stoplight/spectral-cli")
else:
    logger.info("‚úÖ Spectral CLI is ready")

logger.info("=" * 80)
# ========================================


@mcp.tool()
async def validate_openapi(spec_path: str, ruleset: str = None) -> Dict:
    """
    Validate OpenAPI specifications against Spectral rules and LLM semantic analysis.

    Args:
        spec_path: Path to OpenAPI specification file
        ruleset: Optional path to custom Spectral ruleset

    Returns:
        Validation results with violations, severity summary, and suggested fixes
    """
    try:
        # Validate input
        input_data = ValidateOpenAPIInput(spec_path=spec_path, ruleset=ruleset)

        # Resolve paths
        spec_path_obj = Path(input_data.spec_path).resolve()
        if not spec_path_obj.exists():
            return {
                "violations": [],
                "severity_summary": {"critical": 1, "warning": 0, "info": 0},
                "suggested_fixes": [f"File not found: {spec_path}"],
            }

        # Determine ruleset path
        if input_data.ruleset:
            ruleset_path = str(Path(input_data.ruleset).resolve())
        else:
            # Use default ruleset from project root
            ruleset_path = str(project_root / "rules" / "spectral_ruleset.yaml")

        # Run governance scan
        project_path = str(spec_path_obj.parent)
        scanner = GovernanceScanner(
            project_path=project_path,
            ruleset_path=ruleset_path,
            llm_endpoint="http://localhost:11434",
        )

        # Execute scan (single file target)
        result = await scanner.scan(output_path=None, target_spec=str(spec_path_obj))

        # Combine all violations
        all_violations = result.spectral_results + result.llm_results

        # Normalize violations
        normalized_violations = [
            OutputNormalizer.normalize_violation(v) for v in all_violations
        ]

        # Calculate summary
        severity_summary = OutputNormalizer.calculate_severity_summary(
            normalized_violations
        )
        suggested_fixes = OutputNormalizer.extract_suggested_fixes(
            normalized_violations
        )

        # Build output
        output = ValidateOpenAPIOutput(
            violations=normalized_violations,
            severity_summary=severity_summary,
            suggested_fixes=suggested_fixes,
        )

        return output.model_dump()

    except Exception as e:
        # Return error as a critical violation
        return {
            "violations": [
                {
                    "rule_id": "mcp-error",
                    "description": f"Error during validation: {str(e)}",
                    "severity": "critical",
                    "file": spec_path,
                    "line": None,
                    "fix_hint": "Check server logs for details",
                }
            ],
            "severity_summary": {"critical": 1, "warning": 0, "info": 0},
            "suggested_fixes": ["Fix the error and retry"],
        }


@mcp.tool()
async def validate_architecture(
    project_path: str, rule_profile: str = "default"
) -> Dict:
    """
    Validate Java architecture using ArchUnit rules.

    Args:
        project_path: Path to compiled Java classes directory
        rule_profile: Rule profile to use (default: 'default')

    Returns:
        Validation results with violations, impacted layers, and refactoring guidance
    """
    try:
        # Validate input
        input_data = ValidateArchitectureInput(
            project_path=project_path, rule_profile=rule_profile
        )

        # Resolve path
        project_path_obj = Path(input_data.project_path).resolve()
        if not project_path_obj.exists():
            return {
                "violations": [
                    {
                        "rule_id": "path-error",
                        "description": f"Project path not found: {project_path}",
                        "severity": "critical",
                        "file": project_path,
                        "line": None,
                        "fix_hint": "Verify the path exists and contains compiled Java classes",
                    }
                ],
                "impacted_layers": [],
                "refactoring_guidance": ["Fix the path error"],
            }

        # Run ArchUnit scan
        engine = ArchUnitEngine(str(project_path_obj))
        violations = engine.run_scan()

        # Enhance with LLM if available
        # User requested to skip LLM during scan and use it for fixes instead
        # if violations:
        #     try:
        #         llm = LLMAnalyzer(api_endpoint="http://localhost:11434")
        #         violations = await llm.enhance_arch_violations(violations)
        #     except Exception:
        #         pass  # Continue without LLM

        # Normalize violations
        normalized_violations = [
            OutputNormalizer.normalize_violation(v) for v in violations
        ]

        # Extract architectural information
        impacted_layers = OutputNormalizer.extract_impacted_layers(
            normalized_violations
        )
        refactoring_guidance = OutputNormalizer.generate_refactoring_guidance(
            normalized_violations
        )

        # Build output
        output = ValidateArchitectureOutput(
            violations=normalized_violations,
            impacted_layers=impacted_layers,
            refactoring_guidance=refactoring_guidance,
        )

        return output.model_dump()

    except Exception as e:
        return {
            "violations": [
                {
                    "rule_id": "mcp-error",
                    "description": f"Error during validation: {str(e)}",
                    "severity": "critical",
                    "file": project_path,
                    "line": None,
                    "fix_hint": "Check server logs for details",
                }
            ],
            "impacted_layers": [],
            "refactoring_guidance": ["Fix the error and retry"],
        }


@mcp.tool()
async def governance_summary(project_path: str, spec_path: str = None) -> Dict:
    """
    Provide overall governance health summary for a project.

    Args:
        project_path: Path to project directory
        spec_path: Optional path to OpenAPI spec file (supports .yaml, .yml, .json).
                   If not provided and no specs auto-detected, will return a prompt
                   asking user to provide the spec path.

    Returns:
        Overall health score, violation counts, and recommended next steps.
        If spec_path is needed but not provided, returns a special prompt response.
    """
    try:
        # Validate input
        input_data = GovernanceSummaryInput(project_path=project_path)

        # Resolve path
        project_path_obj = Path(input_data.project_path).resolve()
        if not project_path_obj.exists():
            return {
                "overall_health_score": 0,
                "critical_violations": 1,
                "warnings": 0,
                "next_steps": [f"Project path not found: {project_path}"],
                "scan_types_detected": [],
                "needs_spec_path": False,
            }

        # Detect project type
        detector = ProjectDetector(str(project_path_obj))

        scan_types = []
        all_violations = []

        # Check for OpenAPI specs
        specs = []
        if spec_path:
            # User provided explicit spec path - validate it
            spec_path_obj = Path(spec_path)
            if not spec_path_obj.is_absolute():
                spec_path_obj = project_path_obj / spec_path_obj

            if not spec_path_obj.exists():
                return {
                    "overall_health_score": 0,
                    "critical_violations": 1,
                    "warnings": 0,
                    "next_steps": [
                        f"‚ùå Specified OpenAPI spec not found: {spec_path}",
                        "",
                        "Please check:",
                        "  ‚Ä¢ File path is correct",
                        "  ‚Ä¢ File extension is .json, .yaml, or .yml",
                        "  ‚Ä¢ File exists in the project",
                        "",
                        "üí° Try calling this tool again with the correct spec_path",
                    ],
                    "scan_types_detected": [],
                    "needs_spec_path": True,
                    "spec_path_error": f"File not found: {spec_path}",
                }

            # Validate the spec file
            is_valid, error = detector.validate_spec_syntax(spec_path_obj)
            if not is_valid:
                return {
                    "overall_health_score": 0,
                    "critical_violations": 1,
                    "warnings": 0,
                    "next_steps": [
                        f"‚ùå Invalid OpenAPI specification: {spec_path}",
                        f"   Error: {error}",
                        "",
                        "Please ensure:",
                        "  ‚Ä¢ File is valid JSON or YAML",
                        "  ‚Ä¢ File contains 'openapi' or 'swagger' field",
                        "  ‚Ä¢ File structure is correct",
                        "",
                        "üí° You can validate your spec at: https://editor.swagger.io",
                    ],
                    "scan_types_detected": [],
                    "needs_spec_path": True,
                    "spec_path_error": error,
                }

            specs = [spec_path_obj]
        else:
            # Try auto-detection
            specs = detector.find_openapi_specs()

        # If no specs found and not provided, ask user
        if not specs:
            is_java, build_tool = detector.is_java_project()

            # Check if this is Java-only project or needs spec path
            if is_java:
                # Java project - continue with Java scan only
                scan_types.append("java_architecture")

                # Show helpful message about API specs
                return {
                    "overall_health_score": 0,
                    "critical_violations": 0,
                    "warnings": 0,
                    "next_steps": [
                        f"üìÅ Detected {build_tool} project at: {project_path}",
                        "",
                        "üîç No OpenAPI specification found in standard locations.",
                        "",
                        "If your project has an OpenAPI spec (e.g., in hidden folders like .api-specs/),",
                        "please call this tool again with the spec_path parameter:",
                        "",
                        "Example:",
                        f'  governance_summary(project_path="{project_path}", spec_path=".api-specs/spec-prod.json")',
                        "",
                        "Supported locations for auto-detection:",
                        "  ‚Ä¢ src/main/resources/openapi.{json,yaml,yml}",
                        "  ‚Ä¢ openapi.{json,yaml,yml} (project root)",
                        "  ‚Ä¢ api/openapi.{json,yaml,yml}",
                        "  ‚Ä¢ docs/openapi.{json,yaml,yml}",
                        "",
                        "üìù Running Java architecture scan only...",
                    ],
                    "scan_types_detected": ["java_architecture"],
                    "needs_spec_path": True,
                    "spec_path_prompt": "Please provide the path to your OpenAPI spec file (or press Enter to skip API scanning)",
                }
            else:
                # Not Java, no specs - need user input
                return {
                    "overall_health_score": 0,
                    "critical_violations": 0,
                    "warnings": 0,
                    "next_steps": [
                        f"üìÅ Scanning project at: {project_path}",
                        "",
                        "üîç No OpenAPI specification found.",
                        "üîç Not a Java project (no pom.xml or build.gradle).",
                        "",
                        "Please provide the OpenAPI spec path by calling this tool again:",
                        "",
                        "Example:",
                        f'  governance_summary(project_path="{project_path}", spec_path="path/to/openapi.json")',
                        "",
                        "Supported formats: .json, .yaml, .yml",
                    ],
                    "scan_types_detected": [],
                    "needs_spec_path": True,
                    "spec_path_prompt": "Please provide the path to your OpenAPI spec file",
                }

        # If we have specs, scan them
        if specs:
            scan_types.append("api_spec")

            # Run API spec scan
            try:
                ruleset_path = str(project_root / "rules" / "spectral_ruleset.yaml")

                scanner = GovernanceScanner(
                    project_path=str(project_path_obj),
                    ruleset_path=ruleset_path,
                    llm_endpoint="http://localhost:11434",
                )
                result = await scanner.scan(output_path=None)
                # Skip LLM enhancement for now
                # all_violations.extend(result.spectral_results + result.llm_results)
                all_violations.extend(result.spectral_results)
            except Exception:
                pass  # Continue with other scans

        # Check for Java project
        is_java, build_tool = detector.is_java_project()
        if is_java:
            scan_types.append("java_architecture")

            # Run Java architecture scan
            try:
                engine = ArchUnitEngine(str(project_path_obj))
                violations = engine.run_scan()

                # Enhance with LLM if available
                # User requested to skip LLM during scan
                # if violations:
                #     try:
                #         llm = LLMAnalyzer(api_endpoint="http://localhost:11434")
                #         violations = await llm.enhance_arch_violations(violations)
                #     except Exception:
                #         pass

                all_violations.extend(violations)
            except Exception:
                pass  # Continue

        # Normalize all violations
        normalized_violations = [
            OutputNormalizer.normalize_violation(v) for v in all_violations
        ]

        # Calculate summary
        severity_summary = OutputNormalizer.calculate_severity_summary(
            normalized_violations
        )
        health_score = OutputNormalizer.calculate_health_score(
            severity_summary.critical, severity_summary.warning, severity_summary.info
        )

        # Generate next steps
        next_steps = []
        if severity_summary.critical > 0:
            next_steps.append(
                f"Address {severity_summary.critical} critical violation(s) immediately"
            )
        if severity_summary.warning > 0:
            next_steps.append(f"Review {severity_summary.warning} warning(s)")
        if not next_steps:
            next_steps.append(
                "No violations found. Maintain current quality standards."
            )

        # Build output
        output = GovernanceSummaryOutput(
            overall_health_score=health_score,
            critical_violations=severity_summary.critical,
            warnings=severity_summary.warning,
            next_steps=next_steps,
            scan_types_detected=scan_types,
        )

        result = output.model_dump()
        result["needs_spec_path"] = False  # Scan completed successfully
        return result

    except Exception as e:
        return {
            "overall_health_score": 0,
            "critical_violations": 1,
            "warnings": 0,
            "next_steps": [f"Error during scan: {str(e)}"],
            "scan_types_detected": [],
            "needs_spec_path": False,
        }


# ============================================================================
# AUTO-FIX TOOLS
# ============================================================================

# Global storage for fix sessions (in production, use Redis or database)
_fix_sessions: Dict[str, tuple[AutoFixEngine, List[ProposedFix], ReviewState]] = {}


@mcp.tool()
async def propose_fixes(
    report_path: str,
    project_path: str,
    output_dir: str = None,
    use_copilot: bool = True,
) -> Dict:
    """
    Propose fixes for governance violations from a report.

    Args:
        report_path: Path to governance report JSON file
        project_path: Path to project directory
        output_dir: Optional directory for output files
        use_copilot: Use GitHub Copilot for fast fix generation (default: True, 80-90% faster)

    Returns:
        Proposed fixes with diff previews and metadata
    """
    import time

    start_time = time.time()

    try:
        # Validate paths
        report_file = Path(report_path)
        if not report_file.exists():
            return {
                "fixes_proposed": 0,
                "fixes": [],
                "diff_preview_path": None,
                "safe_to_auto_apply_count": 0,
                "error": f"Report file not found: {report_path}",
            }

        project_dir = Path(project_path)
        if not project_dir.exists():
            return {
                "fixes_proposed": 0,
                "fixes": [],
                "diff_preview_path": None,
                "safe_to_auto_apply_count": 0,
                "error": f"Project directory not found: {project_path}",
            }

        # Initialize engine with Copilot support
        print(f"\n{'='*60}")
        print(
            f"Fix Proposal Engine: {'GitHub Copilot (Fast)' if use_copilot else 'LLM (Legacy)'}"
        )
        print(f"{'='*60}\n")

        engine = AutoFixEngine(str(project_dir), use_copilot=use_copilot)

        # Load violations
        violations = engine.load_governance_report(str(report_file))

        # Propose fixes
        fixes, diffs = await engine.propose_fixes(violations, output_dir)

        elapsed_time = time.time() - start_time
        print(f"\n‚úì Fix proposal completed in {elapsed_time:.2f} seconds")

        if not fixes:
            return {
                "fixes_proposed": 0,
                "fixes": [],
                "diff_preview_path": None,
                "safe_to_auto_apply_count": 0,
                "message": "No fixable violations found",
                "execution_time_seconds": elapsed_time,
            }

        # Create session ID
        import uuid

        session_id = str(uuid.uuid4())

        # Start review session
        review_state = engine.review_gate.start_review(fixes)

        # Store session
        _fix_sessions[session_id] = (engine, fixes, review_state)

        # Convert fixes to output format
        fix_infos = []
        for fix in fixes:
            fix_infos.append(
                ProposedFixInfo(
                    fix_id=fix.fix_id,
                    rule_id=fix.rule_id,
                    file_path=fix.file_path,
                    line_number=fix.line_number,
                    complexity=fix.complexity_level,
                    is_safe_to_auto_apply=fix.is_safe_to_auto_apply,
                    explanation=fix.explanation,
                )
            )

        safe_count = sum(1 for f in fixes if f.is_safe_to_auto_apply)

        # Generate summary table
        table_rows = []
        table_rows.append("| File | Rule | Change Type | Reason |")
        table_rows.append("| :--- | :--- | :--- | :--- |")

        for fix in fixes:
            # Shorten file path for display (cross-platform compatible)
            display_path = Path(fix.file_path).name
            change_type = "AI-Fixed" if "ai" in fix.fix_id else "Rule-Fixed"
            # Get first sentence of explanation
            reason = fix.explanation.split(".")[0] + "."
            table_rows.append(
                f"| {display_path} | {fix.rule_id} | {change_type} | {reason} |"
            )

        summary_table = "\n".join(table_rows)

        # Determine diff preview path
        diff_path = None
        if output_dir:
            diff_path = str(Path(output_dir) / "fix-preview.md")

        output = ProposeFixesOutput(
            fixes_proposed=len(fixes),
            fixes=fix_infos,
            diff_preview_path=diff_path,
            safe_to_auto_apply_count=safe_count,
            summary_table=summary_table,
        )

        result = output.model_dump()
        result["fix_session_id"] = session_id
        result["execution_time_seconds"] = elapsed_time
        result["analyzer_type"] = "GitHub Copilot" if use_copilot else "LLM"
        return result

    except Exception as e:
        elapsed_time = time.time() - start_time
        return {
            "fixes_proposed": 0,
            "fixes": [],
            "diff_preview_path": None,
            "safe_to_auto_apply_count": 0,
            "error": f"Error proposing fixes: {str(e)}",
            "execution_time_seconds": elapsed_time,
        }


@mcp.tool()
async def review_fixes(
    fix_session_id: str,
    approved_fix_ids: List[str] = None,
    rejected_fix_ids: List[str] = None,
    comments: Dict[str, str] = None,
) -> Dict:
    """
    Review proposed fixes by approving or rejecting them.

    Args:
        fix_session_id: Fix session identifier from propose_fixes
        approved_fix_ids: List of fix IDs to approve
        rejected_fix_ids: List of fix IDs to reject
        comments: Optional comments per fix_id

    Returns:
        Review status with counts
    """
    try:
        # Get session
        if fix_session_id not in _fix_sessions:
            return {
                "approved_count": 0,
                "rejected_count": 0,
                "pending_count": 0,
                "ready_to_apply": False,
                "error": f"Invalid session ID: {fix_session_id}",
            }

        engine, fixes, review_state = _fix_sessions[fix_session_id]

        # Apply approvals
        if approved_fix_ids:
            for fix_id in approved_fix_ids:
                engine.review_gate.approve_fix(fix_id)

        # Apply rejections
        if rejected_fix_ids:
            for fix_id in rejected_fix_ids:
                engine.review_gate.reject_fix(fix_id)

        # Add comments
        if comments:
            for fix_id, comment in comments.items():
                engine.review_gate.add_comment(fix_id, comment)

        # Get summary
        summary = review_state.get_summary()

        output = ReviewFixesOutput(
            approved_count=summary["approved"],
            rejected_count=summary["rejected"],
            pending_count=summary["pending"],
            ready_to_apply=summary["approved"] > 0,
        )

        return output.model_dump()

    except Exception as e:
        return {
            "approved_count": 0,
            "rejected_count": 0,
            "pending_count": 0,
            "ready_to_apply": False,
            "error": f"Error reviewing fixes: {str(e)}",
        }


@mcp.tool()
async def create_governance_pr(
    fix_session_id: str, push_to_remote: bool = False, remote_name: str = "origin"
) -> Dict:
    """
    Create pull request information for applied fixes.

    Args:
        fix_session_id: Fix session identifier
        push_to_remote: Whether to push branch to remote
        remote_name: Git remote name

    Returns:
        PR information including title, description, and metadata
    """
    try:
        # Get session
        if fix_session_id not in _fix_sessions:
            return {"error": f"Invalid session ID: {fix_session_id}"}

        engine, fixes, review_state = _fix_sessions[fix_session_id]

        # Check if fixes were applied
        if not hasattr(engine, "pr_creator") or not engine.pr_creator:
            return {"error": "Fixes have not been applied yet. Call apply_fixes first."}

        # Get PR info from last apply_fixes call
        if not engine.last_pr_info:
            return {
                "error": "No PR information available. Ensure fixes were applied with git operations in apply_fixes."
            }

        pr_info = engine.last_pr_info

        # Push to remote if requested
        pushed = False
        if push_to_remote and pr_info.branch_name:
            try:
                engine.pr_creator.push_branch(pr_info.branch_name, remote_name)
                pushed = True
            except Exception:
                # Continue even if push fails
                pass

        # Save PR description
        pr_desc_path = (
            Path(engine.project_path) / "governance-autofix" / "pull-request.md"
        )
        pr_desc_path.parent.mkdir(parents=True, exist_ok=True)
        with open(pr_desc_path, "w", encoding="utf-8") as f:
            f.write(f"# {pr_info.title}\n\n")
            f.write(pr_info.description)

        output = CreateGovernancePROutput(
            branch_name=pr_info.branch_name,
            pr_title=pr_info.title,
            pr_description_path=str(pr_desc_path),
            commits=len(pr_info.commits),
            files_changed=pr_info.files_changed,
            pushed_to_remote=pushed,
        )

        return output.model_dump()

    except Exception as e:
        return {"error": f"Error creating PR: {str(e)}"}


@mcp.tool()
async def run_complete_governance_scan(
    project_path: str = ".", spec_path: str = None
) -> Dict:
    """
    Run a complete governance scan automatically:
    1. Detects project type (API, Java, or both)
    2. Scans OpenAPI specs if found using GovernanceScanner
    3. Runs ArchUnit tests if Java project detected
    4. Automatically merges reports into governance-report.json
    5. Generates fix instructions

    This is the ONE command users need to run - no file names required!

    Args:
        project_path: Path to project directory (default: current directory)
        spec_path: Optional path to OpenAPI spec file (supports .yaml, .yml, .json).
                   If not provided and no specs auto-detected, will prompt user.

    Returns:
        Complete scan results with all violations and generated fix instructions.
        If spec_path is needed, returns with needs_spec_path=True flag.
    """
    try:
        import json
        from pathlib import Path

        project_path_obj = Path(project_path).resolve()
        if not project_path_obj.exists():
            return {
                "error": f"Project path not found: {project_path}",
                "violations": [],
                "fix_instructions_generated": False,
            }

        # Get standardized governance paths (build/governance/)
        paths = get_governance_paths(project_path_obj)

        # Detect project type
        detector = ProjectDetector(str(project_path_obj))

        results = {
            "project_path": str(project_path_obj),
            "governance_dir": str(paths["governance_dir"]),
            "scans_performed": [],
            "violations": [],
            "total_violations": 0,
            "by_type": {},
            "by_severity": {},
            "fix_instructions_generated": False,
            "fix_instructions_path": None,
            "next_steps": [],
            "needs_spec_path": False,  # Set to True only when prompting user
        }

        # Check for OpenAPI specs
        # First, handle explicit spec_path if provided
        if spec_path:
            # User provided explicit spec path - validate it
            spec_path_obj = Path(spec_path)
            if not spec_path_obj.is_absolute():
                spec_path_obj = project_path_obj / spec_path_obj

            if not spec_path_obj.exists():
                return {
                    "project_path": str(project_path_obj),
                    "governance_dir": str(paths["governance_dir"]),
                    "scans_performed": [],
                    "violations": [],
                    "total_violations": 0,
                    "by_type": {},
                    "by_severity": {},
                    "fix_instructions_generated": False,
                    "needs_spec_path": True,
                    "spec_path_error": f"File not found: {spec_path}",
                    "next_steps": [
                        f"‚ùå Specified OpenAPI spec not found: {spec_path}",
                        "",
                        "Please check:",
                        "  ‚Ä¢ File path is correct",
                        "  ‚Ä¢ File extension is .json, .yaml, or .yml",
                        "  ‚Ä¢ File exists in the project",
                        "",
                        "üí° Try calling this tool again with the correct spec_path",
                    ],
                }

            # Validate the spec file
            is_valid, error = detector.validate_spec_syntax(spec_path_obj)
            if not is_valid:
                return {
                    "project_path": str(project_path_obj),
                    "governance_dir": str(paths["governance_dir"]),
                    "scans_performed": [],
                    "violations": [],
                    "total_violations": 0,
                    "by_type": {},
                    "by_severity": {},
                    "fix_instructions_generated": False,
                    "needs_spec_path": True,
                    "spec_path_error": error,
                    "next_steps": [
                        f"‚ùå Invalid OpenAPI specification: {spec_path}",
                        f"   Error: {error}",
                        "",
                        "Please ensure:",
                        "  ‚Ä¢ File is valid JSON or YAML",
                        "  ‚Ä¢ File contains 'openapi' or 'swagger' field",
                        "",
                        "üí° You can validate your spec at: https://editor.swagger.io",
                    ],
                }

            # Valid spec provided - use it
            openapi_specs = [spec_path_obj]
        else:
            # Try auto-detection
            openapi_specs = detector.find_openapi_specs()

        # Check for Java files
        is_java, build_tool = detector.is_java_project()

        api_violations_file = None
        arch_violations_file = None

        # If no specs found and none provided, STOP and require spec_path
        if not openapi_specs:
            warning_msg = {
                "‚ö†Ô∏è  WARNING": "No OpenAPI specs auto-detected",
                "spec_locations_checked": [
                    "src/main/resources/openapi.{json,yaml,yml}",
                    "openapi.{json,yaml,yml} (project root)",
                    "api/openapi.{json,yaml,yml}",
                    "docs/openapi.{json,yaml,yml}",
                    "*openapi*.{json,yaml,yml} (recursive)",
                    "*swagger*.{json,yaml,yml} (recursive)",
                ],
                "how_to_specify": f'run_complete_governance_scan(project_path="{project_path}", spec_path="path/to/spec.json")',
                "supported_formats": [".json", ".yaml", ".yml"],
            }

            # STOP - Do not proceed without OpenAPI spec
            logger.error("‚ùå No OpenAPI specification found")
            logger.error("‚ùå Governance scan REQUIRES an OpenAPI spec file")
            logger.info("")
            logger.info("üìã Locations checked:")
            for loc in warning_msg["spec_locations_checked"]:
                logger.info(f"   ‚Ä¢ {loc}")
            logger.info("")
            logger.info("üí° To proceed, provide the spec_path parameter:")
            logger.info(
                f'   run_complete_governance_scan(project_path="{project_path}", spec_path="path/to/openapi.json")'
            )
            logger.info("")
            logger.info("Or place your spec file in a standard location:")
            logger.info("  ‚Ä¢ src/main/resources/openapi.json")
            logger.info("  ‚Ä¢ openapi.json (project root)")
            logger.info("  ‚Ä¢ api/openapi.json")

            return {
                "project_path": str(project_path_obj),
                "governance_dir": str(paths["governance_dir"]),
                "scans_performed": [],
                "violations": [],
                "total_violations": 0,
                "by_type": {},
                "by_severity": {},
                "fix_instructions_generated": False,
                "needs_spec_path": True,
                "spec_detection_warning": warning_msg,
                "next_steps": [
                    "‚ùå No OpenAPI specification found",
                    "",
                    "üîç Locations checked:",
                    *[f"  ‚Ä¢ {loc}" for loc in warning_msg["spec_locations_checked"]],
                    "",
                    "üìã To proceed, provide the spec_path parameter:",
                    f'   run_complete_governance_scan(project_path="{project_path}", spec_path="path/to/openapi.json")',
                    "",
                    "Or place your spec file in a standard location:",
                    "  ‚Ä¢ src/main/resources/openapi.json",
                    "  ‚Ä¢ openapi.json (project root)",
                    "  ‚Ä¢ api/openapi.json",
                    "",
                    "Supported formats: .json, .yaml, .yml",
                ],
            }

        # 1. Scan OpenAPI if found - USE GovernanceScanner directly
        if openapi_specs:
            results["scans_performed"].append("OpenAPI")
            try:
                # Auto-detect ruleset path
                package_root = Path(__file__).parent.parent.parent
                ruleset_path = package_root / "rules" / "spectral_ruleset.yaml"

                # Create scanner and run scan
                scanner = GovernanceScanner(
                    project_path=str(project_path_obj),
                    ruleset_path=str(ruleset_path),
                    llm_endpoint="http://localhost:11434",
                )

                # Run scan with explicit target spec (first spec in list)
                scan_result = await scanner.scan(
                    output_path=str(paths["api_report_md"]),
                    target_spec=str(openapi_specs[0]),  # ‚Üê Pass the spec path!
                )

                # Check if Spectral actually ran successfully
                # If spectral_results is empty, it might mean Spectral is not installed
                if not scan_result.spectral_results and openapi_specs:
                    logger.warning(
                        "‚ö†Ô∏è  Spectral returned 0 violations - this might indicate Spectral is not installed"
                    )
                    logger.warning(
                        "‚ö†Ô∏è  To install Spectral: npm install -g @stoplight/spectral-cli"
                    )
                    results["warnings"] = results.get("warnings", [])
                    results["warnings"].append(
                        {
                            "type": "spectral_missing",
                            "message": "Spectral CLI not found. Install with: npm install -g @stoplight/spectral-cli",
                            "impact": "OpenAPI spec violations cannot be detected without Spectral",
                        }
                    )

                # The scanner creates governance-report.json in the output directory
                # Check if it was created in the project root (old behavior) and move it
                old_report = project_path_obj / "governance-report.json"
                if old_report.exists():
                    import shutil

                    shutil.move(str(old_report), str(paths["governance_report"]))
                    logger.info(
                        f"Moved governance-report.json to {paths['governance_report']}"
                    )

                # Now load the violations from the governance report
                api_violations_file = paths["governance_report"]
                if api_violations_file.exists():
                    with open(api_violations_file, "r") as f:
                        api_data = json.load(f)
                        api_violation_count = api_data.get(
                            "total_violations", len(api_data.get("violations", []))
                        )
                        results["by_type"]["api"] = api_violation_count

                        # Load violations and ensure they have "type": "api"
                        api_violations = api_data.get("violations", [])
                        for v in api_violations:
                            if "type" not in v:
                                v["type"] = "api"  # Ensure type field exists

                        results["violations"].extend(api_violations)
                        logger.info(
                            f"‚úì Loaded {api_violation_count} API violations from governance report"
                        )
                else:
                    # Fallback: use scan_result directly and add violations
                    results["by_type"]["api"] = scan_result.total_violations
                    # Add Spectral violations to results array
                    for violation in scan_result.spectral_results:
                        violation["type"] = "api"  # Mark as API violation
                        results["violations"].append(violation)
                    logger.warning(
                        f"‚ö†Ô∏è  Governance report not found, using scan_result directly ({scan_result.total_violations} violations)"
                    )

            except Exception as e:
                results["scans_performed"].append(f"OpenAPI (failed: {str(e)})")
                results["error"] = f"OpenAPI scan failed: {str(e)}"
                api_violations_file = None

        # 2. Run ArchUnit if Java project - USE ArchUnitEngine directly
        if is_java:
            results["scans_performed"].append("ArchUnit")
            try:
                # Run ArchUnit scan directly (ensures we get ALL violations, not just what's in a file)
                arch_engine = ArchUnitEngine(str(project_path_obj))
                arch_violations = arch_engine.run_scan()

                results["by_type"]["architecture"] = len(arch_violations)

                # ADD VIOLATIONS TO RESULTS ARRAY!
                results["violations"].extend(arch_violations)

                # Save arch violations to build/governance/arch-violations-temp.json
                arch_violations_file = paths["arch_temp"]
                arch_data = {
                    "violations": arch_violations,
                    "total": len(arch_violations),
                    "by_severity": {},
                }

                # Count by severity
                for v in arch_violations:
                    sev = v.get("severity", "UNKNOWN")
                    arch_data["by_severity"][sev] = (
                        arch_data["by_severity"].get(sev, 0) + 1
                    )

                with open(arch_violations_file, "w") as f:
                    json.dump(arch_data, f, indent=2)

                results["scans_performed"].append(
                    f"ArchUnit ({len(arch_violations)} violations)"
                )

            except Exception as e:
                results["scans_performed"].append(f"ArchUnit (failed: {str(e)})")
                results["error"] = f"ArchUnit scan failed: {str(e)}"
                arch_violations_file = None

        # 3. Merge reports if we have both API and ArchUnit violations
        # Reports are now in build/governance/
        governance_report = paths["governance_report"]

        if arch_violations_file:
            try:
                # If we have both, merge them
                if api_violations_file or governance_report.exists():
                    # Import merge function directly instead of subprocess call
                    # This avoids PATH issues with pipenv not being found
                    import sys

                    package_root = Path(__file__).parent.parent.parent
                    sys.path.insert(0, str(package_root))

                    try:
                        from scripts.merge_reports import merge_reports

                        # Determine input files
                        openapi_input = None
                        if governance_report.exists():
                            openapi_input = str(governance_report)
                        elif api_violations_file:
                            openapi_input = str(api_violations_file)

                        # Call merge function directly (no subprocess needed!)
                        logger.info(
                            f"Merging reports: API={openapi_input}, ArchUnit={arch_violations_file}"
                        )
                        merge_reports(
                            openapi_file=openapi_input,
                            archunit_file=str(arch_violations_file),
                            output_file=str(governance_report),
                        )
                        logger.info(
                            f"‚úì Reports merged successfully into {governance_report}"
                        )

                    except ImportError as ie:
                        logger.error(f"Failed to import merge_reports: {ie}")
                        results["error"] = (
                            "Report merge failed: Could not import merge function"
                        )
                    except Exception as me:
                        logger.error(f"Merge function failed: {me}")
                        results["error"] = f"Report merge failed: {str(me)}"

            except Exception as e:
                results["error"] = f"Report merge failed: {str(e)}"

        # 4. Load or create the final governance report
        # - If API scan ran, governance_report exists with API violations
        # - If only Java scan, we need to create governance_report from arch violations
        # - If both, merge script already created governance_report

        # Check if governance report exists from API or merge
        governance_report_existed = governance_report.exists()

        # Track if we should reload from file
        # - If both API and ArchUnit ran, we have violations in memory from both
        # - Merge script ran and created governance-report.json with BOTH
        # - We should use the merged file (has normalized format with type fields)
        # - If only one scan ran, use memory (file might not exist yet)

        should_load_from_file = False
        if governance_report_existed:
            # If we have violations in memory from both scans, merge script ran
            has_api = results["by_type"].get("api", 0) > 0
            has_arch = results["by_type"].get("architecture", 0) > 0

            if has_api and has_arch:
                # Both scans ran, merge script created the file - use file (has normalized format)
                should_load_from_file = True
                logger.info(
                    "Both API and ArchUnit scans ran - loading merged violations from file"
                )
            elif has_api or has_arch:
                # Only one scan ran - use memory violations (already have them)
                should_load_from_file = False
                logger.info("Single scan type - using violations from memory")
            else:
                # No violations in memory - load from file (edge case)
                should_load_from_file = True
                logger.info("No violations in memory - loading from file")

        if governance_report_existed and should_load_from_file:
            # Report exists from API scan or merge - load it
            try:
                with open(governance_report, "r") as f:
                    report_data = json.load(f)

                    # Load violations from merged file (has proper type fields from merge script)
                    loaded_violations = report_data.get("violations", [])
                    logger.info(
                        f"Loading {len(loaded_violations)} violations from merged governance report"
                    )

                    # Verify type fields exist and log any issues
                    violations_without_type = [
                        v for v in loaded_violations if "type" not in v
                    ]
                    if violations_without_type:
                        logger.warning(
                            f"‚ö†Ô∏è  {len(violations_without_type)} violations missing 'type' field!"
                        )
                        for v in violations_without_type:
                            logger.warning(
                                f"   Missing type: {v.get('rule', 'unknown')}"
                            )

                    # Use file violations (they have proper normalization from merge script)
                    results["violations"] = loaded_violations
                    results["total_violations"] = report_data.get(
                        "total_violations", len(results["violations"])
                    )

                    # Get severity breakdown
                    if "summary" in report_data:
                        results["by_severity"] = report_data["summary"].get(
                            "by_severity", {}
                        )

                # 4.1. Generate a combined markdown report with BOTH API and Architecture violations
                try:
                    combined_md = paths["combined_report"]
                    logger.info(f"Generating combined report: {combined_md}")
                    logger.info(
                        f"Total violations to report: {len(results['violations'])}"
                    )

                    with open(combined_md, "w", encoding="utf-8") as f:
                        f.write("# Complete Governance Scan Report\n\n")
                        f.write(
                            f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                        )
                        f.write(f"**Report Location:** `{paths['governance_dir']}`\n\n")

                        f.write("## Executive Summary\n\n")
                        f.write(f"- **Project Path:** `{project_path_obj}`\n")
                        f.write(
                            f"- **Total Violations:** {results['total_violations']}\n"
                        )
                        f.write(
                            f"  - API Violations: {results['by_type'].get('api', 0)}\n"
                        )
                        f.write(
                            f"  - Architecture Violations: {results['by_type'].get('architecture', 0)}\n\n"
                        )

                        # API Violations Section
                        api_violations = [
                            v for v in results["violations"] if v.get("type") == "api"
                        ]
                        logger.info(f"API violations to write: {len(api_violations)}")

                        if api_violations:
                            f.write(f"## API Violations ({len(api_violations)})\n\n")
                            for idx, v in enumerate(api_violations):
                                try:
                                    severity_icon = (
                                        "üî¥" if v.get("severity", 1) == 0 else "üü°"
                                    )
                                    f.write(
                                        f"{severity_icon} **{v.get('rule', 'unknown')}**\n"
                                    )
                                    f.write(
                                        f"  - Message: {v.get('message', 'No message')}\n"
                                    )
                                    f.write(f"  - File: `{v.get('file', 'unknown')}`\n")
                                    f.write(f"  - Line: {v.get('line', 'N/A')}\n\n")
                                except Exception as ve:
                                    logger.error(
                                        f"Error writing API violation {idx}: {ve}"
                                    )
                                    logger.error(f"Violation data: {v}")
                        else:
                            logger.warning("No API violations found to write")

                        # Architecture Violations Section
                        arch_violations = [
                            v
                            for v in results["violations"]
                            if v.get("type") == "architecture"
                        ]
                        logger.info(
                            f"Architecture violations to write: {len(arch_violations)}"
                        )

                        if arch_violations:
                            f.write(
                                f"## Architecture Violations ({len(arch_violations)})\n\n"
                            )
                            for idx, v in enumerate(arch_violations):
                                try:
                                    severity_icon = (
                                        "üî¥" if v.get("severity", 1) == 0 else "üü°"
                                    )
                                    f.write(
                                        f"{severity_icon} **{v.get('rule', 'unknown')}**\n"
                                    )
                                    f.write(
                                        f"  - Message: {v.get('message', 'No message')}\n"
                                    )
                                    f.write(f"  - File: `{v.get('file', 'unknown')}`\n")
                                    f.write(f"  - Line: {v.get('line', 'N/A')}\n\n")
                                except Exception as ve:
                                    logger.error(
                                        f"Error writing architecture violation {idx}: {ve}"
                                    )
                                    logger.error(f"Violation data: {v}")
                        else:
                            logger.warning("No architecture violations found to write")

                        f.write("## Next Steps\n\n")
                        f.write(
                            f"1. Review fix instructions: `{paths['fix_instructions_json']}`\n"
                        )
                        f.write("2. Apply fixes by category for better control\n")
                        f.write("3. Apply all fixes at once\n")

                    logger.info(
                        f"‚úì Combined report generated successfully: {combined_md}"
                    )
                    results["combined_report_path"] = str(combined_md)

                except Exception as e:
                    # Log the error but don't fail the scan
                    logger.error(f"‚ùå Failed to generate combined markdown report: {e}")
                    logger.error(f"Violations in results: {len(results['violations'])}")
                    logger.error(f"Results structure: {list(results.keys())}")
                    import traceback

                    logger.error(traceback.format_exc())

            except Exception as e:
                results["error"] = f"Failed to load governance report: {str(e)}"
        else:
            # No governance report exists - create it from violations array
            # This happens for Java-only scans where we have arch violations but no API scan
            if len(results["violations"]) > 0:
                results["total_violations"] = len(results["violations"])

                # Create the governance report from violations array
                governance_data = {
                    "violations": results["violations"],
                    "total_violations": results["total_violations"],
                    "summary": {
                        "by_type": results["by_type"],
                        "by_severity": results["by_severity"],
                    },
                    "timestamp": datetime.now().isoformat(),
                }

                # Save it (DON'T reload it - we already have the data in results)
                with open(governance_report, "w") as f:
                    json.dump(governance_data, f, indent=2)

        # 5. Generate fix instructions automatically if we have violations
        # Check violations array length instead of relying on governance_report file
        if len(results["violations"]) > 0:
            try:
                # Use VSCodeCopilotEngine directly instead of subprocess
                from engines.vscode_copilot_engine import VSCodeCopilotEngine

                # Load violations from governance report in build/governance/
                with open(governance_report, "r") as f:
                    report_data = json.load(f)
                    violations = report_data.get("violations", [])

                # Generate fix instructions
                engine = VSCodeCopilotEngine(project_path=str(project_path_obj))
                instructions = engine.prepare_fix_instructions(violations)

                # Save fix instructions to build/governance/copilot-fixes/
                fix_output_dir = paths["fix_instructions_dir"]
                fix_output_dir.mkdir(parents=True, exist_ok=True)

                fix_instructions_md = paths["fix_instructions_md"]
                fix_instructions_json = paths["fix_instructions_json"]

                # Generate markdown document
                markdown_doc = engine.generate_copilot_workspace_instructions(
                    instructions
                )
                with open(fix_instructions_md, "w", encoding="utf-8") as f:
                    f.write(markdown_doc)

                # Save JSON for automation
                json_data = [
                    {
                        "fix_id": f"fix-{idx:04d}",
                        "rule_id": inst.rule_id,
                        "file": inst.file_path,
                        "lines": inst.context_lines,
                        "prompt": inst.to_copilot_prompt(),
                        "complexity": inst.fix_strategy.complexity.value,
                        "safety": inst.fix_strategy.safety.value,
                        "related_files": inst.related_files,
                        "secondary_fixes": (
                            [  # ‚≠ê NEW: Include controller updates
                                {
                                    "file": fix.file_path,
                                    "type": fix.change_type,
                                    "reason": fix.reason,
                                    "old_code": fix.old_code,
                                    "new_code": fix.new_code,
                                    "instruction": fix.instruction,
                                }
                                for fix in inst.secondary_fixes
                            ]
                            if inst.secondary_fixes
                            else []
                        ),
                    }
                    for idx, inst in enumerate(instructions, 1)
                ]
                with open(fix_instructions_json, "w") as f:
                    json.dump(json_data, f, indent=2)

                # Calculate statistics
                safe_count = sum(
                    1
                    for inst in instructions
                    if inst.fix_strategy.safety.value == "SAFE"
                )
                review_count = sum(
                    1
                    for inst in instructions
                    if inst.fix_strategy.safety.value == "REVIEW_REQUIRED"
                )
                manual_count = sum(
                    1
                    for inst in instructions
                    if inst.fix_strategy.safety.value == "MANUAL_ONLY"
                )

                # Set results
                results["fix_instructions_generated"] = True
                results["fix_instructions_path"] = str(fix_instructions_md)
                results["fix_summary"] = {
                    "total_fixes": len(instructions),
                    "safe_fixes": safe_count,
                    "review_required": review_count,
                    "manual_only": manual_count,
                }

                # Build next_steps - include spec warning if present
                next_steps_list = []

                # Add spec detection warning first if it was skipped
                if results.get("spectral_skipped"):
                    next_steps_list.extend(
                        [
                            "‚ö†Ô∏è  IMPORTANT: OpenAPI spec scanning was skipped",
                            f"   Reason: {results.get('spectral_skip_reason', 'No spec found')}",
                            "",
                        ]
                    )

                next_steps_list.extend(
                    [
                        f"‚úÖ Scan complete! Found {results['total_violations']} violations",
                        f"   - API violations: {results['by_type'].get('api', 0)}",
                        f"   - Architecture violations: {results['by_type'].get('architecture', 0)}",
                        f"‚úÖ Generated {len(instructions)} fix instructions",
                        f"   - SAFE fixes: {safe_count}",
                        f"   - REVIEW_REQUIRED: {review_count}",
                        f"   - MANUAL_ONLY: {manual_count}",
                        f"üìÅ Project path: {project_path_obj}",
                        f"üìÑ Fix instructions: {fix_instructions_json}",
                        f"üìÑ Governance report: {governance_report}",
                        "‚ö° Apply all fixes automatically",
                        "‚ö° Apply fixes for specific category",
                        f"üîç Review fixes: Open {fix_instructions_md}",
                    ]
                )

                # Add spec path hint at the end if spec was skipped
                if results.get("spectral_skipped"):
                    next_steps_list.extend(
                        [
                            "",
                            "üí° To include API spec scanning in next run:",
                            f'   run_complete_governance_scan(project_path="{project_path}", spec_path="path/to/openapi.json")',
                        ]
                    )

                results["next_steps"] = next_steps_list

            except Exception as e:
                results["fix_instructions_generated"] = False

                # Build next_steps with spec warning if applicable
                next_steps_list = []
                if results.get("spectral_skipped"):
                    next_steps_list.extend(
                        [
                            "‚ö†Ô∏è  IMPORTANT: OpenAPI spec scanning was skipped",
                            f"   Reason: {results.get('spectral_skip_reason', 'No spec found')}",
                            "",
                        ]
                    )

                next_steps_list.extend(
                    [
                        f"‚úÖ Scan complete! Found {results['total_violations']} violations",
                        f"   - API violations: {results['by_type'].get('api', 0)}",
                        f"   - Architecture violations: {results['by_type'].get('architecture', 0)}",
                        f"‚ùå Fix generation failed: {str(e)}",
                        f"üìÑ View report: {governance_report}",
                    ]
                )

                if results.get("spectral_skipped"):
                    next_steps_list.extend(
                        [
                            "",
                            "üí° To include API spec scanning in next run:",
                            f'   run_complete_governance_scan(project_path="{project_path}", spec_path="path/to/openapi.json")',
                        ]
                    )

                results["next_steps"] = next_steps_list
        else:
            results["next_steps"] = [
                "‚úÖ No violations found! Your project is compliant.",
                "üéâ Great job maintaining governance standards!",
            ]

        return results

    except Exception as e:
        return {
            "error": f"Complete scan failed: {str(e)}",
            "violations": [],
            "fix_instructions_generated": False,
        }


@mcp.tool()
async def apply_fixes(category: str = None, subcategory: str = None) -> Dict:
    """
    üéØ **PRIMARY TOOL FOR APPLYING ALL FIXES** üéØ

    ‚ö†Ô∏è  IMPORTANT: This is THE ONLY tool you should use for applying fixes!
    ‚ö†Ô∏è  DO NOT use apply_category_fixes or any other experimental tools!
    ‚ö†Ô∏è  This tool handles BOTH all-fixes AND category-filtered fixes!

    Use this tool when user says:
    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    ‚úÖ "apply fixes" ‚Üí applies ALL fixes (no category filter)
    ‚úÖ "apply all fixes" ‚Üí applies ALL fixes (no category filter)
    ‚úÖ "fix all violations" ‚Üí applies ALL fixes (no category filter)

    ‚úÖ "apply fixes for RESOURCE_NAMING" ‚Üí applies ONLY that category
    ‚úÖ "fix architecture issues" ‚Üí applies ARCHITECTURE category
    ‚úÖ "fix code quality" ‚Üí applies CODE_QUALITY category

    ‚úÖ "apply fixes for Plural Resource Names" ‚Üí applies ONLY that subcategory
    ‚úÖ "fix kebab case paths" ‚Üí applies specific subcategory
    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

    Available Categories (use exact names):
    - RESOURCE_NAMING (Priority 1) - Plural resources, kebab-case, no verbs
    - ARCHITECTURE (Priority 2) - Layered architecture, dependencies
    - CODE_QUALITY (Priority 3) - Logging, exceptions, injection
    - SECURITY (Priority 4) - Authentication, credentials
    - DATA_TYPES (Priority 5) - UUID IDs, camelCase, ISO8601
    - HTTP_SEMANTICS (Priority 6) - Status codes, HTTP methods
    - PAGINATION (Priority 7) - Page-based pagination
    - RESPONSE_STRUCTURE (Priority 8) - Response envelopes
    - DOCUMENTATION (Priority 9) - API descriptions
    - OTHER (Priority 10) - Miscellaneous

    Args:
        category: Optional category name to filter fixes (case-insensitive)
                 Examples: "RESOURCE_NAMING", "ARCHITECTURE", "CODE_QUALITY"
                 If None or empty, applies ALL fixes.
        subcategory: Optional subcategory name to filter fixes further
                    Examples: "Plural Resources", "Kebab Case Paths", "No Field Injection"
                    Only used if category is also specified.

    Returns:
        - List of fixes to apply (filtered by category/subcategory if specified)
        - Ultra-strict instructions for Copilot to apply every fix
        - Validation happens automatically after
        - Progress tracking if category used
    """
    try:
        from pathlib import Path
        import json

        # Look for instructions file
        possible_paths = [
            Path.cwd()
            / "build"
            / "governance"
            / "copilot-fixes"
            / "copilot-fix-instructions.json",
            Path.cwd() / "copilot-fixes" / "copilot-fix-instructions.json",
        ]

        instructions_file = None
        for path in possible_paths:
            if path.exists():
                instructions_file = path
                break

        if not instructions_file:
            return {
                "error": "‚ùå Fix instructions not found. Run 'scan project' first.",
                "success": False,
                "total_fixes": 0,
            }

        # Load all fix instructions
        with open(instructions_file, "r") as f:
            all_instructions = json.load(f)

        # Filter by category if specified
        if category:
            category_manager = CategoryManager()
            category_upper = category.upper()

            # Validate category exists
            if category_upper not in category_manager.CATEGORIES:
                available_categories = list(category_manager.CATEGORIES.keys())
                return {
                    "error": f"‚ùå Invalid category: {category}",
                    "success": False,
                    "available_categories": available_categories,
                    "hint": f"Use one of: {', '.join(available_categories)}",
                }

            # Get rules for this category
            category_info = category_manager.CATEGORIES[category_upper]
            category_rules = category_info.rules

            # If subcategory specified, filter further
            if subcategory:
                subcategory_manager = SubcategoryManager()

                # Find the subcategory's rule_id by looking through SUBCATEGORIES class attribute
                subcategory_rule_id = None
                subcategory_definition = None

                # SUBCATEGORIES is a dict with rule_id as key
                for rule_id, subcat_def in SubcategoryManager.SUBCATEGORIES.items():
                    if subcat_def.category == category_upper and (
                        rule_id == subcategory or subcat_def.display_name == subcategory
                    ):
                        subcategory_rule_id = rule_id
                        subcategory_definition = subcat_def
                        break

                if not subcategory_rule_id:
                    # Get available subcategories for this category
                    available_subcats = [
                        subcat.display_name
                        for rule_id, subcat in SubcategoryManager.SUBCATEGORIES.items()
                        if subcat.category == category_upper
                    ]
                    return {
                        "error": f"‚ùå Invalid subcategory '{subcategory}' for category {category_upper}",
                        "success": False,
                        "category": category_upper,
                        "available_subcategories": available_subcats,
                        "hint": f"Use one of: {', '.join(available_subcats)}",
                    }

                # Filter instructions to only this subcategory's rule
                filtered_instructions = [
                    fix
                    for fix in all_instructions
                    if fix.get("rule_id") == subcategory_rule_id
                ]

                if not filtered_instructions:
                    return {
                        "error": f"‚ùå No fixes found for subcategory: {subcategory}",
                        "success": False,
                        "category": category_upper,
                        "subcategory": subcategory,
                        "rule_checked": subcategory_rule_id,
                    }

                fixes_to_apply = filtered_instructions
                category_display = (
                    f" for subcategory '{subcategory}' in {category_upper}"
                )
            else:
                # Filter instructions to only this category's rules
                filtered_instructions = [
                    fix
                    for fix in all_instructions
                    if fix.get("rule_id") in category_rules
                ]

                if not filtered_instructions:
                    return {
                        "error": f"‚ùå No fixes found for category: {category}",
                        "success": False,
                        "category": category_upper,
                        "rules_checked": list(category_rules),
                    }

                fixes_to_apply = filtered_instructions
                category_display = f" for category {category_upper}"
        else:
            fixes_to_apply = all_instructions
            category_display = ""

        total_fixes = len(fixes_to_apply)

        # ========================================
        # V4+ ULTRA-STRICT COPILOT INSTRUCTIONS
        # ========================================

        return {
            "success": True,
            "total_fixes": total_fixes,
            "category_filter": category.upper() if category else "ALL",
            "instructions_file": str(instructions_file),
            "fixes": fixes_to_apply,
            "üö®_COPILOT_READ_THIS_FIRST": f"""
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üö® CRITICAL REQUIREMENT: APPLY ALL {total_fixes} FIXES{category_display} WITHOUT EXCEPTION üö®
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

YOU ARE ABSOLUTELY REQUIRED TO:
‚úÖ 1. Apply fix-0001 through fix-{total_fixes:04d} - ALL {total_fixes} FIXES{category_display}
‚úÖ 2. Use 'replace_string_in_file' tool for EACH fix
‚úÖ 3. Read each file completely using 'read_file' tool first
‚úÖ 4. Apply EVERY fix regardless of complexity/effort/risk label
‚úÖ 5. Continue without asking permission - JUST DO ALL {total_fixes} FIXES
‚úÖ 6. After ALL {total_fixes} fixes: call validate_fixes() automatically

THESE PHRASES ARE ABSOLUTELY FORBIDDEN:
‚ùå "remaining fixes require manual intervention"
‚ùå "these fixes are too complex for me to apply"
‚ùå "let me stop here and check with you"
‚ùå "should I continue with the remaining fixes?"
‚ùå "these require architecture decisions"

WORKFLOW (REPEAT FOR ALL {total_fixes} FIXES):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Step 1: read_file(file_path, start=1, end=9999)
Step 2: Find the EXACT code that violates the rule
Step 3: Generate fix based on the 'prompt' field
Step 4: replace_string_in_file(old_code, fixed_code)
Step 5: Move to NEXT fix immediately - NO PAUSING
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

SUCCESS CRITERIA:
üéØ Applied: {total_fixes}/{total_fixes} fixes
üéØ Anything less than {total_fixes} = FAILURE
üéØ Target: 100% completion rate

START NOW - BEGIN WITH FIX-0001 AND CONTINUE UNTIL FIX-{total_fixes:04d}
NO STOPPING. NO ASKING. JUST APPLY ALL {total_fixes} FIXES.
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
            """,
            "IMMEDIATE_ACTION_REQUIRED": f"""
START WITH FIX-0001:
- File: {all_instructions[0]['file']}
- Rule: {all_instructions[0]['rule_id']}
- Action: read_file ‚Üí find violation ‚Üí replace_string_in_file

THEN CONTINUE: fix-0002, fix-0003, ... fix-{total_fixes:04d}

AFTER ALL {total_fixes} FIXES: call validate_fixes()
            """,
            "performance_metrics": {
                "total_fixes_to_apply": total_fixes,
                "minimum_acceptable": total_fixes,
                "success_threshold": 1.0,  # 100% required
                "failure_if_less_than": total_fixes,
                "current_progress": "0/{total_fixes}",
                "target_progress": f"{total_fixes}/{total_fixes}",
            },
            "fix_0001_details": {
                "fix_id": all_instructions[0]["fix_id"],
                "file": all_instructions[0]["file"],
                "rule": all_instructions[0]["rule_id"],
                "prompt": all_instructions[0]["prompt"],
                "complexity": all_instructions[0].get("complexity", "unknown"),
                "safety": all_instructions[0].get("safety", "unknown"),
                "NOTE": "Start with this fix, then continue to fix-0002, fix-0003, etc.",
            },
        }

    except Exception as e:
        return {
            "error": f"‚ùå Failed to apply fixes: {str(e)}",
            "success": False,
            "total_fixes": 0,
        }


@mcp.tool()
async def validate_fixes(
    category: str = None, project_path: str = ".", clean_build: bool = True
) -> Dict:
    """
    üß™ UNIFIED VALIDATION TOOL - Validates fixes with tests + re-scan

    This tool performs COMPREHENSIVE validation after applying fixes:

    PHASE 1: BUILD & TEST VALIDATION
    ‚úÖ Detects build system (Gradle/Maven/npm/pytest)
    ‚úÖ Runs build to compile code changes
    ‚úÖ Executes all unit tests and integration tests
    ‚úÖ Reports test pass/fail status

    PHASE 2: GOVERNANCE RE-SCAN (if category specified)
    ‚úÖ Re-runs ArchUnit tests with compiled code
    ‚úÖ Re-runs Spectral scan for API specs
    ‚úÖ Compares violations before/after
    ‚úÖ Calculates violations fixed vs new

    PHASE 3: CONFIDENCE REPORT
    ‚úÖ Calculates overall confidence score
    ‚úÖ Provides actionable next steps
    ‚úÖ Saves detailed validation report

    Args:
        category: Optional category to re-scan (e.g., "ARCHITECTURE", "CODE_QUALITY")
                 If specified, performs governance re-scan and comparison
                 If None, only runs tests without re-scanning
        project_path: Path to project (default: current directory)
        clean_build: Whether to run clean before build (default: True)

    Returns:
        Comprehensive validation results including:
        - Test results (pass/fail)
        - Build status
        - Violations before/after (if category specified)
        - Confidence score
        - Next steps
    """
    try:
        from pathlib import Path
        import json
        import subprocess
        from autofix.build_validator import BuildValidator
        from scanner.project_detector import ProjectDetector

        project_path_obj = Path(project_path).resolve()

        results = {
            "project_path": str(project_path_obj),
            "category": category.upper() if category else None,
            "build_system": None,
            "validation_phases": {
                "build_and_test": {},
                "governance_rescan": {},
                "confidence_report": {},
            },
            "success": False,
            "confidence_score": 0,
            "next_steps": [],
        }

        # ========================================
        # PHASE 1: BUILD & TEST VALIDATION
        # ========================================
        print("\n" + "=" * 80)
        print("PHASE 1: BUILD & TEST VALIDATION")
        print("=" * 80)

        # Detect build system
        detector = ProjectDetector(str(project_path_obj))
        is_java, build_tool = detector.is_java_project()

        has_package_json = (project_path_obj / "package.json").exists()
        has_requirements = (project_path_obj / "requirements.txt").exists()
        has_poetry = (project_path_obj / "pyproject.toml").exists()

        test_results = {
            "tests_run": [],
            "tests_passed": 0,
            "tests_failed": 0,
            "tests_skipped": 0,
            "failures": [],
            "build_success": False,
            "build_duration": 0,
        }

        # Run build based on detected system
        if is_java and build_tool:
            results["build_system"] = build_tool
            test_results["tests_run"].append(f"{build_tool} test")

            try:
                if build_tool == "gradle":
                    # Use 'test' task for faster validation
                    # The 'test' task compiles and runs tests but doesn't run static analysis
                    # (unless explicitly configured in build.gradle to depend on check)
                    gradle_cmd = ["./gradlew", "test", "--no-daemon", "--console=plain"]
                    if not (project_path_obj / "gradlew").exists():
                        gradle_cmd = ["gradle", "test", "--console=plain"]

                    print(f"üî® Running: {' '.join(gradle_cmd)}")
                    import time

                    start_time = time.time()

                    result = subprocess.run(
                        gradle_cmd,
                        cwd=str(project_path_obj),
                        capture_output=True,
                        text=True,
                        timeout=600,  # 10 minute timeout (increased for large projects)
                    )

                    test_results["build_duration"] = time.time() - start_time
                    output = result.stdout + result.stderr

                    # Check exit code instead of parsing output (more reliable)
                    if result.returncode == 0:
                        test_results["tests_passed"] += 1
                        test_results["build_success"] = True
                        print(
                            f"‚úÖ Gradle tests passed ({test_results['build_duration']:.1f}s)"
                        )
                    else:
                        test_results["tests_failed"] += 1
                        # Extract failure information from output
                        for line in output.split("\n"):
                            if (
                                "FAILED" in line
                                or "error" in line.lower()
                                or "failure" in line.lower()
                            ):
                                test_results["failures"].append(line.strip())
                        print(
                            f"‚ùå Gradle tests failed ({test_results['build_duration']:.1f}s)"
                        )
                        print(f"   Exit code: {result.returncode}")

                elif build_tool == "maven":
                    # Use 'test' phase for faster validation (skip static analysis)
                    mvn_cmd = [
                        "./mvnw",
                        "test",
                        "-B",
                        "-Dspotbugs.skip=true",
                        "-Dpmd.skip=true",
                        "-Dcheckstyle.skip=true",
                        "-Ddependency-check.skip=true",
                    ]
                    if not (project_path_obj / "mvnw").exists():
                        mvn_cmd = [
                            "mvn",
                            "test",
                            "-B",
                            "-Dspotbugs.skip=true",
                            "-Dpmd.skip=true",
                            "-Dcheckstyle.skip=true",
                            "-Ddependency-check.skip=true",
                        ]

                    print("üî® Running: mvn test (skipping static analysis)")
                    import time

                    start_time = time.time()

                    result = subprocess.run(
                        mvn_cmd,
                        cwd=str(project_path_obj),
                        capture_output=True,
                        text=True,
                        timeout=600,  # 10 minute timeout (increased for large projects)
                    )

                    test_results["build_duration"] = time.time() - start_time
                    output = result.stdout + result.stderr

                    # Check exit code instead of parsing output (more reliable)
                    if result.returncode == 0:
                        test_results["tests_passed"] += 1
                        test_results["build_success"] = True
                        print(
                            f"‚úÖ Maven tests passed ({test_results['build_duration']:.1f}s)"
                        )
                    else:
                        test_results["tests_failed"] += 1
                        # Extract failure information from output
                        for line in output.split("\n"):
                            if "FAILURE" in line or "ERROR" in line or "Failed" in line:
                                test_results["failures"].append(line.strip())
                        print(
                            f"‚ùå Maven tests failed ({test_results['build_duration']:.1f}s)"
                        )
                        print(f"   Exit code: {result.returncode}")

            except subprocess.TimeoutExpired:
                test_results["build_success"] = False
                test_results["failures"].append("Build timed out after 10 minutes")
                print("‚è±Ô∏è Build timed out after 10 minutes")
            except Exception as e:
                test_results["build_success"] = False
                test_results["failures"].append(str(e))
                print(f"‚ùå Build failed: {e}")

        elif has_package_json:
            results["build_system"] = "npm"
            test_results["tests_run"].append("npm test")
            # Similar npm test logic...
            test_results["build_success"] = True  # Simplified for now

        elif has_requirements or has_poetry:
            results["build_system"] = "pytest"
            test_results["tests_run"].append("pytest")
            # Similar pytest logic...
            test_results["build_success"] = True  # Simplified for now

        results["validation_phases"]["build_and_test"] = test_results

        # ========================================
        # PHASE 2: GOVERNANCE RE-SCAN (if category specified)
        # ========================================
        governance_results = None

        if category and test_results["build_success"]:
            print("\n" + "=" * 80)
            print(f"PHASE 2: GOVERNANCE RE-SCAN FOR CATEGORY: {category.upper()}")
            print("=" * 80)

            try:
                # Verify category is valid
                category_upper = category.upper()
                if category_upper not in CategoryManager.CATEGORIES:
                    governance_results = {
                        "error": f"Invalid category: {category}",
                        "valid_categories": list(CategoryManager.CATEGORIES.keys()),
                    }
                else:
                    # Load violations before fix
                    paths = get_governance_paths(project_path_obj)
                    governance_report = paths["governance_report"]

                    if governance_report.exists():
                        with open(governance_report, "r") as f:
                            report_data = json.load(f)

                        violations_before = report_data.get("violations", [])

                        # Filter to category
                        manager = CategoryManager()
                        category_violations_before = [
                            v
                            for v in violations_before
                            if manager.categorize_violation(v) == category_upper
                        ]

                        # Initialize validator and run re-scan
                        validator = BuildValidator(str(project_path_obj))

                        # Run governance re-scan
                        print(f"üîç Re-scanning for {category_upper} violations...")
                        validation_result = validator.validate_fixes(
                            category=category_upper,
                            violations_before=category_violations_before,
                            clean_build=False,  # Already built in Phase 1
                            output_dir=str(paths["governance_dir"]),
                        )

                        # Save validation report
                        validator.save_validation_report(
                            validation_result,
                            output_path=str(
                                project_path_obj / "validation-report.json"
                            ),
                        )

                        governance_results = {
                            "category": validation_result.category,
                            "violations_before": validation_result.violations_before,
                            "violations_after": validation_result.violations_after,
                            "violations_fixed": validation_result.violations_fixed,
                            "new_violations": validation_result.new_violations,
                            "success": validation_result.success,
                            "message": validation_result.message,
                        }

                        print(f"\n{validation_result.message}")

                    else:
                        governance_results = {
                            "error": "No baseline governance report found",
                            "message": "Run initial scan before validating fixes",
                        }

            except Exception as e:
                governance_results = {
                    "error": f"Governance re-scan failed: {str(e)}",
                    "success": False,
                }

        results["validation_phases"]["governance_rescan"] = governance_results or {
            "skipped": not category
        }

        # ========================================
        # PHASE 3: CONFIDENCE REPORT
        # ========================================
        print("\n" + "=" * 80)
        print("PHASE 3: CONFIDENCE REPORT")
        print("=" * 80)

        # Calculate overall confidence score
        confidence_score = 0

        # Factor 1: Build success (40 points)
        if test_results["build_success"]:
            confidence_score += 40

        # Factor 2: Tests passed (30 points)
        if test_results["tests_passed"] > 0 and test_results["tests_failed"] == 0:
            confidence_score += 30

        # Factor 3: Governance improvements (30 points) - if category specified
        if governance_results and not governance_results.get("skipped"):
            if governance_results.get("success"):
                if governance_results.get("violations_fixed", 0) > 0:
                    confidence_score += 30
                elif governance_results.get("violations_after", 0) == 0:
                    confidence_score += 30
                elif governance_results.get("new_violations", 0) == 0:
                    confidence_score += 15
        else:
            # If no category specified, tests alone give full confidence
            if test_results["build_success"] and test_results["tests_passed"] > 0:
                confidence_score += 30

        results["confidence_score"] = confidence_score
        results["success"] = confidence_score >= 70

        # Generate next steps
        if confidence_score >= 90:
            results["next_steps"] = [
                "‚úÖ EXCELLENT! All validations passed with high confidence",
                "üìä Confidence: HIGH (90+) - Changes are safe to commit",
                (
                    "üéØ Build: ‚úÖ Passed | Tests: ‚úÖ Passed | Governance: ‚úÖ Improved"
                    if category
                    else "üéØ Build: ‚úÖ Passed | Tests: ‚úÖ Passed"
                ),
                "‚úÖ Commit: git add . && git commit -m 'Fixed governance violations'",
                "üöÄ Push: git push origin main",
            ]
        elif confidence_score >= 70:
            results["next_steps"] = [
                "‚úÖ GOOD! Most validations passed",
                "üìä Confidence: MEDIUM-HIGH (70-89) - Changes look safe",
                "üîç Review: Double-check modified files",
                "‚úÖ Commit when ready: git add . && git commit -m 'Fixed violations'",
            ]
        elif confidence_score >= 40:
            results["next_steps"] = [
                "‚ö†Ô∏è  CAUTION: Some validations failed",
                "üìä Confidence: MEDIUM (40-69) - Review carefully",
                (
                    f"üîç Issues: {len(test_results['failures'])} test failures"
                    if test_results["failures"]
                    else "Governance re-scan shows remaining issues"
                ),
                "üîß Fix: Address failures before committing",
            ]
        else:
            results["next_steps"] = [
                "‚ùå STOP: Critical validation failures",
                "üìä Confidence: LOW (<40) - Do NOT commit yet",
                f"üîç Build: {'‚ùå Failed' if not test_results['build_success'] else '‚úÖ Passed'}",
                (
                    f"üîç Tests: {test_results['tests_failed']} failed"
                    if test_results["tests_failed"] > 0
                    else ""
                ),
                "üîß REQUIRED: Fix all failures before proceeding",
            ]

        results["validation_phases"]["confidence_report"] = {
            "score": confidence_score,
            "rating": (
                "HIGH"
                if confidence_score >= 90
                else (
                    "MEDIUM-HIGH"
                    if confidence_score >= 70
                    else "MEDIUM" if confidence_score >= 40 else "LOW"
                )
            ),
            "recommendation": (
                "COMMIT"
                if confidence_score >= 70
                else "REVIEW" if confidence_score >= 40 else "FIX_FIRST"
            ),
        }

        # Print summary
        print(f"\nüìä Overall Confidence Score: {confidence_score}/100")
        print(
            f"üéØ Recommendation: {results['validation_phases']['confidence_report']['recommendation']}"
        )
        print("=" * 80 + "\n")

        return results

    except Exception as e:
        import traceback

        traceback.print_exc()
        return {
            "error": f"‚ùå Validation failed: {str(e)}",
            "success": False,
            "confidence_score": 0,
        }


@mcp.tool()
async def get_violations_by_category(project_path: str = ".") -> Dict:
    """
    Get violations organized by category/categories for incremental fixing.

    Use this when user says:
    - "Get violations by category"
    - "Get violations by categories"
    - "Show me violations by category"
    - "Categorize violations"
    - "Break down violations by category"

    This tool helps manage large numbers of violations by breaking them into
    logical categories (Resource Naming, Data Types, HTTP Semantics, etc.)
    that can be tackled one at a time.

    Args:
        project_path: Path to project (default: current directory)

    Returns:
        Violations organized by category with priority and effort estimates
    """
    import json

    try:
        project_path_obj = Path(project_path).resolve()
        paths = get_governance_paths(project_path_obj)

        # Load governance report
        governance_report = paths["governance_report"]
        if not governance_report.exists():
            return {
                "error": "No governance report found. Run scan first.",
                "suggestion": "Use run_complete_governance_scan() first",
            }

        with open(governance_report, "r") as f:
            report_data = json.load(f)

        violations = report_data.get("violations", [])

        if not violations:
            return {"message": "No violations found", "categories": {}}

        # Categorize violations
        manager = CategoryManager()
        summary = manager.generate_category_summary(violations)

        # Organize by subcategories
        subcategory_manager = SubcategoryManager()
        subcategory_summary = subcategory_manager.generate_subcategory_summary(
            violations
        )

        # Build response
        result = {
            "total_violations": len(violations),
            "total_categories": len([c for c in summary.values() if c["count"] > 0]),
            "categories": {},
            "recommended_order": [],
        }

        # Format categories with subcategories
        for category_name, category_data in summary.items():
            if category_data["count"] == 0:
                continue

            # Get subcategories for this category from subcategory_summary
            category_subcategories = {}
            if category_name in subcategory_summary:
                subcat_data = subcategory_summary[category_name].get(
                    "subcategories", {}
                )
                for rule_id, subcat_info in subcat_data.items():
                    category_subcategories[subcat_info["display_name"]] = {
                        "display_name": subcat_info["display_name"],
                        "rule_id": rule_id,
                        "count": subcat_info["violation_count"],
                        "fix_complexity": subcat_info["fix_complexity"],
                        "example": subcat_info["example"],
                        "violations": [
                            {
                                "rule": v.get("rule") or v.get("rule_id"),
                                "message": v.get("message"),
                                "file": v.get("file") or v.get("file_path"),
                                "line": v.get("line") or v.get("line_number"),
                                "severity": v.get("severity"),
                            }
                            for v in subcat_info["violations"]
                        ],
                    }

            result["categories"][category_name] = {
                "display_name": category_data["display_name"],
                "description": category_data["description"],
                "count": category_data["count"],
                "priority": category_data["priority"],
                "effort": category_data["effort"],
                "subcategories": category_subcategories,
                "violations": [
                    {
                        "rule": v.get("rule") or v.get("rule_id"),
                        "message": v.get("message"),
                        "file": v.get("file") or v.get("file_path"),
                        "line": v.get("line") or v.get("line_number"),
                        "severity": v.get("severity"),
                    }
                    for v in category_data["violations"]
                ],
            }

        # Add recommended order
        for cat_name, cat_data in sorted(
            summary.items(), key=lambda x: x[1]["priority"]
        ):
            if cat_data["count"] > 0:
                priority_label = (
                    "üî¥ HIGH"
                    if cat_data["priority"] <= 3
                    else "üü° MEDIUM" if cat_data["priority"] <= 6 else "üü¢ LOW"
                )
                result["recommended_order"].append(
                    {
                        "priority_label": priority_label,
                        "category": cat_name,
                        "display_name": cat_data["display_name"],
                        "count": cat_data["count"],
                        "effort": cat_data["effort"],
                    }
                )

        # Add workflow tips
        result["workflow_tips"] = [
            "‚úÖ Fix categories one at a time for better focus",
            "‚úÖ Each category is broken down into subcategories by rule type",
            "‚úÖ Start with highest priority (üî¥) categories first",
            "‚úÖ Target specific subcategories for even more granular fixes",
            "‚úÖ Review and commit after each category or subcategory",
            "üí° You can apply fixes:",
            "   ‚Ä¢ All at once",
            "   ‚Ä¢ By category (e.g., RESOURCE_NAMING)",
            "   ‚Ä¢ By subcategory (e.g., Plural Resource Names)",
            "üí° Incremental fixing prevents token limit issues",
            "üí° Subcategories help identify which specific rules need attention",
        ]

        return result

    except Exception as e:
        return {"error": f"Failed to categorize violations: {str(e)}"}


@mcp.tool()
async def get_next_category_to_fix(project_path: str = ".") -> Dict:
    """
    Get the next recommended category to fix based on priority.

    This tool helps you work through violations incrementally by suggesting
    which category to tackle next.

    Args:
        project_path: Path to project (default: current directory)

    Returns:
        Recommended next category with fix instructions
    """
    import json

    try:
        project_path_obj = Path(project_path).resolve()
        paths = get_governance_paths(project_path_obj)

        # Load governance report
        governance_report = paths["governance_report"]
        if not governance_report.exists():
            return {
                "error": "No governance report found. Run scan first.",
                "suggestion": "Use run_complete_governance_scan() first",
            }

        with open(governance_report, "r") as f:
            report_data = json.load(f)

        violations = report_data.get("violations", [])

        if not violations:
            return {
                "message": "üéâ No violations found! All governance rules are passing.",
                "status": "complete",
            }

        # Get next category
        manager = CategoryManager()
        next_category, next_violations = manager.get_next_category_to_fix(
            violations, []
        )

        if not next_category:
            return {
                "message": "All categories have been addressed",
                "status": "complete",
            }

        category_info = CategoryManager.CATEGORIES[next_category]

        return {
            "recommended_category": next_category,
            "priority": category_info.priority,
            "effort": category_info.estimated_effort,
            "violation_count": len(next_violations),
            "sample_violations": [
                {
                    "rule": v.get("code") or v.get("rule_id"),
                    "message": v.get("message"),
                    "file": v.get("source") or v.get("file"),
                }
                for v in next_violations[:5]  # Show first 5
            ],
            "next_steps": [
                f"‚úÖ Recommended: Apply fixes for {next_category} category",
                f"üìñ Description: {category_info.description}",
                f"‚ö° Priority: {category_info.priority} | Effort: {category_info.estimated_effort}",
            ],
        }

    except Exception as e:
        return {"error": f"Failed to determine next category: {str(e)}"}


if __name__ == "__main__":
    # Run MCP server with stdio transport
    mcp.run(transport="stdio")
